{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/teddylee777/langchain-kr/blob/main/17-LangGraph/02-langgraph-groundcheck.ipynb\n",
    "- https://www.youtube.com/watch?v=4JdzuB702wI&t=1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Retrieval GroundCheck\n",
    "기존의 RAG에 GroundChecker 기능을 추가하여 답변-문서 간에 관련성 여부를 확인합니다.\n",
    "\n",
    "이번 튜토리얼에서는 직접 구현한 LLM 으로 GroundChecker를 만들어서 관련성 여부를 체크 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api key\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangSmith 설졍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=\"ls__708b8970829247d1a055f33c434aad1d\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"edu-langchain-0326\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorDB RAG 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.pdf import PDFRetrievalChain\n",
    "\n",
    "# PDF 문서를 vector로 변환하여 retriever 생성\n",
    "pdf = PDFRetrievalChain([\"data/SPRI_AI_Brief_2023년12월호_F.pdf\"]).create_chain()\n",
    "pdf_retriever = pdf.retriever\n",
    "pdf_chain = pdf.chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphState\n",
    "각 노드에서 다음 노드로 전달되는 상태를 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "# GraphState 상태를 저장하는 용도로 사용합니다.\n",
    "class GraphState(TypedDict):\n",
    "    question: str  # 질문\n",
    "    context: str  # 문서의 검색 결과\n",
    "    answer: str  # 답변\n",
    "    relevance: str  # 답변의 문서에 대한 관련성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 노드와 엣지\n",
    "주요 개념\n",
    "\n",
    "- GraphState(상태 저장 그래프): LangGraph는 그래프의 각 노드가 계산의 단계를 나타내며, 그래프는 계산이 진행됨에 따라 전달되고 업데이트되는 상태를 유지하는 상태 저장 그래프 개념을 중심으로 작동합니다.\n",
    "- Node(노드): 노드는 LangGraph의 구성 요소입니다. 각 노드는 함수 또는 계산 단계를 나타냅니다. 입력 처리, 의사 결정, 외부 API와의 상호 작용 등 특정 작업을 수행하도록 노드를 정의할 수 있습니다.\n",
    "- Edge(엣지): 에지는 그래프에서 노드를 연결하여 계산의 흐름을 정의합니다. LangGraph는 조건부 에지를 지원하므로 그래프의 현재 상태에 따라 실행할 다음 노드를 동적으로 결정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.utils import format_docs\n",
    "from langchain_upstage import UpstageGroundednessCheck\n",
    "\n",
    "os.environ[\"UPSTAGE_API_KEY\"] = \"up_gBj0DmykHNhYdGNmB4JLWT7iVlKVF\"\n",
    "\n",
    "# 업스테이지 문서 관련성 체크 기능을 설정합니다. https://upstage.ai\n",
    "upstage_ground_checker = UpstageGroundednessCheck()\n",
    "\n",
    "# 업스테이지 문서 관련성 체크를 실행합니다.\n",
    "upstage_ground_checker.run(\n",
    "    {\n",
    "        \"context\": format_docs(\n",
    "            pdf_retriever.invoke(\"삼성전자가 개발한 생성AI 의 이름은\")\n",
    "        ),\n",
    "        \"answer\": \"삼성전자가 개발한 생성AI 의 이름은 `빅스비 AI` 입니다.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "def Get_LLM():    \n",
    "    os.environ[\"AZURE_OPENAI_API_KEY\"] = '352a6bee97b5451ab5866993a7ef4ce4'\n",
    "    os.environ[\"AZURE_OPENAI_ENDPOINT\"] = 'https://aoai-spn-krc.openai.azure.com/'\n",
    "    model = AzureChatOpenAI(  \n",
    "      api_version = '2024-02-01',\n",
    "      azure_deployment = 'gpt-4o-kr-spn',\n",
    "      temperature = 0.0\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 프롬프트 생성 - 다음 맥락을 기반으로만 질문에 답하십시오.\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = Get_LLM()\n",
    "\n",
    "test_chain = (\n",
    "    {\"context\": pdf_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# _question = \"삼성에서 개발한 AI는?\"\n",
    "# _question = \"가우스를 개발한 회사의 2023년 매출액은?\"\n",
    "# _question = \"가우스는 언제 공개되었나?\"\n",
    "_question = \"가우스를 개발한 회사의 이름과 매출액은?\"\n",
    "\n",
    "test_chain.invoke(_question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상태 저장 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# 관련성 결과 형식\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Boolean values to check for relevance on retrieved documents.\"\"\"\n",
    "\n",
    "    score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'True' or 'False'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "node 함수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.utils import format_docs\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from rag.base import RetrievalChain\n",
    "\n",
    "# 벡터DB에서 문서를 검색합니다.\n",
    "def retrieve_document(state: GraphState) -> GraphState:\n",
    "    # Question 에 대한 문서 검색을 retriever 로 수행합니다.\n",
    "    retrieved_docs = pdf_retriever.invoke(state[\"question\"])\n",
    "    # 검색된 문서를 context 키에 저장합니다.\n",
    "    return GraphState(context=format_docs(retrieved_docs))\n",
    "\n",
    "\n",
    "# Chain을 사용하여 답변을 생성합니다.\n",
    "def llm_answer(state: GraphState) -> GraphState:\n",
    "    return GraphState(\n",
    "        answer=pdf_chain.invoke(\n",
    "            {\"question\": state[\"question\"], \"context\": state[\"context\"]}\n",
    "        ),\n",
    "        context=state[\"context\"],\n",
    "        question=state[\"question\"],\n",
    "    )\n",
    "\n",
    "\n",
    "# 테스트용 더미 답변 생성 함수\n",
    "def fake_llm_answer(state: GraphState) -> GraphState:\n",
    "    return GraphState(\n",
    "        answer=\"삼성전자가 개발한 생성AI 의 이름은 `빅스비 AI` 입니다.\",\n",
    "        context=state[\"context\"],\n",
    "        question=state[\"question\"],\n",
    "    )\n",
    "\n",
    "\n",
    "# 관련성 체크를 함수\n",
    "def relevance_check(state: GraphState) -> GraphState:\n",
    "    doc = state[\"context\"]\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # 시스템 프롬프트:\n",
    "    # 당신은 사용자의 질문과 관련된 문서의 중요성을 평가하는 평가자입니다. \\n\n",
    "    # 문서가 질문과 관련된 키워드나 의미를 포함하고 있다면, 해당 문서를 관련성이 있는 것으로 평가하세요. \\n\n",
    "    # 문서가 질문과 관련이 있는지 여부를 나타내기 위해 'True' 또는 'False'라는 불린 점수를 부여하세요.\n",
    "\n",
    "    system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "        Give a boolean score 'True' or 'False' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "    grade_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    llm = Get_LLM()\n",
    "    structured_llm = llm.with_structured_output(GradeDocuments)\n",
    "    grader_llm = grade_prompt | structured_llm\n",
    "    result = grader_llm.invoke({\"document\": doc, \"question\": question})\n",
    "    return GraphState(\n",
    "        relevance=result.score,\n",
    "        context=state[\"context\"],\n",
    "        answer=state[\"answer\"],\n",
    "        question=state[\"question\"],\n",
    "    )\n",
    "\n",
    "# 관련성 체크 결과를 반환합니다.\n",
    "def is_relevant(state: GraphState) -> GraphState:\n",
    "    if state[\"relevance\"].lower() == \"true\":\n",
    "        return \"관련성 O\"\n",
    "    else:\n",
    "        return \"관련성 X\"\n",
    "\n",
    "# 관련성 체크를 실행합니다.\n",
    "# def relevance_check(state: GraphState) -> GraphState:\n",
    "#     # 관련성 체크를 실행합니다. 결과: grounded, notGrounded, notSure\n",
    "#     response = upstage_ground_checker.run(\n",
    "#         {\"context\": state[\"context\"], \"answer\": state[\"answer\"]}\n",
    "#     )\n",
    "#     return GraphState(\n",
    "#         relevance=response,\n",
    "#         context=state[\"context\"],\n",
    "#         answer=state[\"answer\"],\n",
    "#         question=state[\"question\"],\n",
    "#     )\n",
    "\n",
    "\n",
    "# 관련성 체크 결과를 반환합니다.\n",
    "# def is_relevant(state: GraphState) -> GraphState:\n",
    "#     if state[\"relevance\"] == \"grounded\":\n",
    "#         return \"관련성 O\"\n",
    "#     elif state[\"relevance\"] == \"notGrounded\":\n",
    "#         return \"관련성 X\"\n",
    "#     elif state[\"relevance\"] == \"notSure\":\n",
    "#         return \"확인불가\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 그래프 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# langgraph.graph에서 StateGraph와 END를 가져옵니다.\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# 노드들을 정의합니다.\n",
    "workflow.add_node(\"retrieve\", retrieve_document)  # 에이전트 노드를 추가합니다.\n",
    "workflow.add_node(\"llm_answer\", llm_answer)  # 정보 검색 노드를 추가합니다.\n",
    "\n",
    "# 테스트용 fake llm 을 추가합니다.\n",
    "# workflow.add_node(\"llm_answer\", fake_llm_answer)  # 정보 검색 노드를 추가합니다.\n",
    "\n",
    "\n",
    "workflow.add_node(\n",
    "    \"relevance_check\", relevance_check\n",
    ")  # 답변의 문서에 대한 관련성 체크 노드를 추가합니다.\n",
    "\n",
    "# 각 노드들을 연결합니다.\n",
    "workflow.add_edge(\"retrieve\", \"llm_answer\")  # 검색 -> 답변\n",
    "workflow.add_edge(\"llm_answer\", \"relevance_check\")  # 답변 -> 관련성 체크\n",
    "\n",
    "# 조건부 엣지를 추가합니다.\n",
    "workflow.add_conditional_edges(\n",
    "    \"relevance_check\",  # 관련성 체크 노드에서 나온 결과를 is_relevant 함수에 전달합니다.\n",
    "    is_relevant,\n",
    "    {\n",
    "        \"관련성 O\": END,  # 관련성이 있으면 종료합니다.\n",
    "        \"관련성 X\": \"retrieve\",  # 관련성이 없으면 다시 답변을 생성합니다.\n",
    "    },\n",
    ")\n",
    "\n",
    "# 시작점을 설정합니다.\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "\n",
    "# 기록을 위한 메모리 저장소를 설정합니다.\n",
    "memory = MemorySaver()\n",
    "\n",
    "# 그래프를 컴파일합니다.\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 그래프 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(\n",
    "        Image(app.get_graph(xray=True).draw_mermaid_png())\n",
    "    )  # 실행 가능한 객체의 그래프를 mermaid 형식의 PNG로 그려서 표시합니다. xray=True는 추가적인 세부 정보를 포함합니다.\n",
    "except:\n",
    "    # 이 부분은 추가적인 의존성이 필요하며 선택적으로 실행됩니다.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.pregel import GraphRecursionError\n",
    "\n",
    "def execute_graph(app, inputs, config):\n",
    "    try:\n",
    "        result = app.invoke(inputs, config)\n",
    "        return result\n",
    "    except GraphRecursionError as e:\n",
    "        print(f\"Encountered a recursion error: {e}\")\n",
    "        # Custom error handling logic here\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# recursion_limit: 최대 반복 횟수, thread_id: 실행 ID (구분용)\n",
    "config = RunnableConfig(recursion_limit=13, configurable={\"thread_id\": \"SELF-RAG\"})\n",
    "\n",
    "_question = \"삼성에서 개발한 LLM\"\n",
    "# _question = \"AI 가우스를 만든 회사 매출액\"\n",
    "\n",
    "# GraphState 객체를 활용하여 질문을 입력합니다.\n",
    "inputs = GraphState(question=_question)\n",
    "# output = app.invoke(inputs, config=config)\n",
    "output = execute_graph(app, inputs, config)\n",
    "\n",
    "if output is not None:\n",
    "    # 출력 결과를 확인합니다.\n",
    "    print(\"Question: \\t\", output[\"question\"])\n",
    "    print(\"Answer: \\t\", output[\"answer\"])\n",
    "    print(\"Relevance: \\t\", output[\"relevance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from langgraph.errors import GraphRecursionError\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(recursion_limit=13, configurable={\"thread_id\": \"SELF-RAG\"})\n",
    "\n",
    "_question = \"삼성에서 개발한 LLM\"\n",
    "# _question = \"AI 가우스를 만든 회사 매출액\"\n",
    "\n",
    "# GraphState 객체를 활용하여 질문을 입력합니다.\n",
    "inputs = GraphState(question=_question)\n",
    "\n",
    "# app.stream을 통해 입력된 메시지에 대한 출력을 스트리밍합니다.\n",
    "try:\n",
    "    for output in app.stream(inputs, config=config):\n",
    "        # 출력된 결과에서 키와 값을 순회합니다.\n",
    "        for key, value in output.items():\n",
    "            # 노드의 이름과 해당 노드에서 나온 출력을 출력합니다.\n",
    "            pprint.pprint(f\"[NODE] {key}\")\n",
    "            for k, v in value.items():\n",
    "                pprint.pprint(f\"<{k}> {v}\")\n",
    "            pprint.pprint(\"===\" * 10)\n",
    "            # 출력 값을 예쁘게 출력합니다.\n",
    "            # pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "except GraphRecursionError as e:\n",
    "    pprint.pprint(f\"Recursion limit reached: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure_analy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
